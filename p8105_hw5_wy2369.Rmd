---
title: "p8105_hw5_wy2369"
output: github_document
date: "2022-11-13"
author: Wenjing Yang
---

```{r, include=FALSE}
library(tidyverse)
library(viridis)
library(patchwork)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

### Import the data

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. 

As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

### Tidy the data

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

### Make a plot

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

# Problem 2

### Read and clean

Using the code chunk to read and clean data from `homicide-data.csv`

```{r}
homicide_data = 
  read_csv("./data/homicide-data.csv") %>% 
  janitor::clean_names()
```

### Describe the raw data

In the raw data, there are **`r nrow(homicide_data)`** rows and **`r ncol(homicide_data)`** columns. It contains **`r ncol(homicide_data)`** important variables which are uid (`uid`), dates of report (`reported_date`), last and first names of victim (`victim_last`and `victim_first`), race of victim (`victim_race`), age of victim (`victim_age`), sex of victim (`victim_sex`), variables of location (`city`, `state`, `lat` and `lon` ), and the status of case (`disposition`) from 50 large cities in the U.S.

### Create a `city_state` variable

Using mutate function to create a new variable. 

```{r}
homicide_df = 
  homicide_data %>% 
  mutate(city_state = str_c(city, ",", state)) 
```

### Find the total number of homicides within cities.

```{r}
homicide_df %>% 
  group_by(city_state) %>% 
  count(city_state) %>% 
  summarize(n) 
```

From this table, we could find the total number of homicides within 51 cities not 50, and there is one uncommon data from **Tulsa,AL**. Since I know **Tulsa,AL** does not exist in the U.S, we should probably ignore this data from `homicide_df` dataset. 

### Find the number of unsolved homicides

```{r}
tidy_homicide_df = 
  homicide_df %>% 
  filter(city_state != "Tulsa,AL") %>% 
  mutate(
    homicide_status =  ifelse(disposition != "Closed by arrest", "unsolved", "solved")
  )


Unsolve_data = 
  tidy_homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    unsolved = sum(homicide_status == "unsolved"),
    total_homicides = n())

Unsolve_data %>% 
    knitr::kable(col.names = c("city, state", "unsolved homicides", "total homicides"))
```

From this table, we could find the total number of homicides within 50 cities and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

### Estimate the proportion of homicides that are unsolved

Use the `prop.test` function to estimate the proportion of homicides that are unsolved for the city of Baltimore, MD. ANd I pull the estimated proportion and confidence intervals (CI) from the resulting tidy dataframe.

```{r}
Baltimore_data = 
  Unsolve_data %>%
  filter(city_state == "Baltimore,MD")

  
Baltimore_test = 
  prop.test(
    x = Baltimore_data %>% pull(unsolved),
    n = Baltimore_data %>% pull(total_homicides)) %>% 
  broom::tidy() %>% 
  select(estimate,conf.low,conf.high)

Baltimore_test
```

The test results tell us that the estimate proportion of homicides that are unsolved for the city of Baltimore, MD is **0.646** and the 95% confidence interval is **(0.628, 0.663)**. 

### Estimate the proportion for all 50 cities. 

```{r}
all_test = 
  Unsolve_data %>% 
  mutate(
    proportion = map2(.x = unsolved, .y = total_homicides, ~prop.test(x = .x, n = .y)),
    proportion = map(proportion, broom::tidy)
  ) %>% 
  unnest() %>% 
  select(city_state, unsolved, total_homicides, estimate, conf.low,conf.high)
  
all_test %>% 
  knitr::kable(
    digits = 3,
    col.names = c("city, state", "unsolved cases", "total cases", "estimate proportion", "lower CI", "upper CI"))

```

### Make a plot shows the estimates and CIs for each city. 

Using this code chunk to create a plot shows the estimates and CIs for each city, and add error bars based on the upper and lower limits.

```{r}
all_test %>% 
  mutate(
   city_state = fct_reorder(city_state,estimate) 
  ) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "The estimates and CIs for each city with errorbar",
       x = "City, State",
       y = "Estimate Proportion") 

```

# Problem 3

### Write a function

Using this code chunk to write a function that use `t.test` to calculate the estimate and p_value.

```{r}
fn_t_test = function(n_obs =30, mu = 0, sigma = 5){
  
  fn_data = rnorm(n = n_obs,mean = mu, sd = sigma)
  
  tibble(fn_data)
  
  fn_data %>% 
    t.test() %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
}
```

### Repeat for mean = {0,1,2,3,4,5,6}

Using iteration to repeat the function and make a dataset `test_results_df` includes mu, iteration, estimate, p.value, and reject_null (1 for reject Ho; 0 for fail to reject Ho).

```{r}
test_results_df = 
  expand_grid(
    mu = c(0:6),
    iteration = 1:5000) %>%
  mutate(
    estimate_df =
      map(.x = mu, ~fn_t_test(mu = .x))
  ) %>%
  unnest(estimate_df) %>% 
  mutate(
    reject_null = ifelse(p.value < 0.05, 1, 0))
```

### Make a plot showing the association between effect size and power

This plot shows the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mean on the x axis. 

```{r}
test_results_df %>% 
  group_by(mu) %>% 
  summarize(n_total = n(), n_reject = sum(reject_null==1)) %>% 
  ggplot(aes(x = mu, y = n_reject/n_total)) +
  geom_point()+
  geom_line()+
  labs(
    title = "Association between effect size and power",
    x = "The true value of mean",
    y = "Proportion of times the null was rejected"
  ) 
```

Based on the plot, proportion of times the null was rejected increases as the true value of mean increases, which means the power of the test increases as true mean increases. Thus I think the association between effect size and power is that power of the test will increases as effect size increases. 

### Make two plots showing average estimate of mean vs true mean 

Make a plot `plot_all` showing the average estimate of mu_hat on the y axis and the true value of mu on the x axis.

Make a second plot `plot_reject` showing the average estimate of mu_hat only in samples for which the null was rejected on the y axis and the true value of mu on the x axis. 

```{r}
plot_all = 
test_results_df %>% 
  group_by(mu) %>% 
  summarize(mean_mu = mean(estimate)) %>%
  ggplot(aes(x = mu, y = mean_mu)) +
  geom_point()+
  geom_line()+
  labs(
    title = "Average estimate of mean vs true mean",
    x = "The true value of mean",
    y = "The average estimate of mean"
  ) +
  theme(plot.title = element_text(size = 8),text = element_text(size = 8))


plot_reject =
test_results_df %>% 
  group_by(mu) %>% 
  filter(reject_null ==1) %>% 
  summarize(mean_mu_reject = mean(estimate)) %>%
  ggplot(aes(x = mu, y = mean_mu_reject)) +
  geom_point()+
  geom_line()+
  labs(
    title = "Average estimate of mean vs true mean when null was rejected",
    x = "The true value of mean",
    y = "The average estimate of mean"
  ) +
  theme(plot.title = element_text(size = 8),text = element_text(size = 8))

plot_all + plot_reject
```

Based on the plots shown above, the sample average of mu_hat across tests for which the null is rejected is not approximately equal to the true value of mu. When true mean is equal to 0~2, the average estimate of mean is higher than the true mean. When true mean is equal to 3~6, the average estimate of mean is approximately equal to the true mean. 





