---
title: "p8105_hw5_wy2369"
output: github_document
date: "2022-11-13"
author: Wenjing Yang
---

```{r, include=FALSE}
library(tidyverse)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

### Import the data

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. 

As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

### Tidy the data

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

### Make a plot

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

# Problem 2

### Read and clean

Using the code chunk to read and clean data from `homicide-data.csv`

```{r}
homicide_data = 
  read_csv("./data/homicide-data.csv") %>% 
  janitor::clean_names()
```

### Describe the raw data

In the raw data, there are **`r nrow(homicide_data)`** rows and **`r ncol(homicide_data)`** columns. It contains **`r ncol(homicide_data)`** important variables which are uid (`uid`), dates of report (`reported_date`), last and first names of victim (`victim_last`and `victim_first`), race of victim (`victim_race`), age of victim (`victim_age`), sex of victim (`victim_sex`), variables of location (`city`, `state`, `lat` and `lon` ), and the status of case (`disposition`) from 50 large cities in the U.S.

### Create a `city_state` variable

Using mutate function to create a new variable. 

```{r}
homicide_df = 
  homicide_data %>% 
  mutate(city_state = str_c(city, ",", state)) 
```

### Find the total number of homicides within cities.

```{r}
homicide_df %>% 
  group_by(city_state) %>% 
  count(city_state) %>% 
  summarize(n) 
```

From this table, we could find the total number of homicides within 51 cities not 50, and there is one uncommon data from **Tulsa,AL**. Since I know **Tulsa,AL** does not exist in the U.S, we should probably ignore this data from `homicide_df` dataset. 

### Find the number of unsolved homicides

```{r}
tidy_homicide_df = 
  homicide_df %>% 
  filter(city_state != "Tulsa,AL") %>% 
  mutate(
    homicide_status =  ifelse(disposition != "Closed by arrest", "unsolved", "solved")
  )


Unsolve_data = 
  tidy_homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    unsolved = sum(homicide_status == "unsolved"),
    total_homicides = n())

Unsolve_data %>% 
    knitr::kable(col.names = c("city, state", "unsolved homicides", "total homicides"))
```

From this table, we could find the total number of homicides within 50 cities and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

### Estimate the proportion of homicides that are unsolved

Use the `prop.test` function to estimate the proportion of homicides that are unsolved for the city of Baltimore, MD. ANd I pull the estimated proportion and confidence intervals (CI) from the resulting tidy dataframe.

```{r}
Baltimore_data = 
  Unsolve_data %>%
  filter(city_state == "Baltimore,MD")

  
Baltimore_test = 
  prop.test(
    x = Baltimore_data %>% pull(unsolved),
    n = Baltimore_data %>% pull(total_homicides)) %>% 
  broom::tidy() %>% 
  select(estimate,conf.low,conf.high)

Baltimore_test
```

The test results tell us that the estimate proportion of homicides that are unsolved for the city of Baltimore, MD is **0.646** and the 95% confidence interval is **(0.628, 0.663)**. 

### Estimate the proportion for all 50 cities. 

```{r}
all_test = 
  Unsolve_data %>% 
  mutate(
    proportion = map2(.x = unsolved, .y = total_homicides, ~prop.test(x = .x, n = .y)),
    proportion = map(proportion, broom::tidy)
  ) %>% 
  unnest() %>% 
  select(city_state, unsolved, total_homicides, estimate, conf.low,conf.high)
  
all_test %>% 
  knitr::kable(
    digits = 3,
    col.names = c("city, state", "unsolved cases", "total cases", "estimate proportion", "lower CI", "upper CI"))

```

### Make a plot shows the estimates and CIs for each city. 

Using this code chunk to create a plot shows the estimates and CIs for each city, and add error bars based on the upper and lower limits.

```{r}
all_test %>% 
  mutate(
   city_state = fct_reorder(city_state,estimate) 
  ) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "The estimates and CIs for each city with errorbar",
       x = "City, State",
       y = "Estimate Proportion") 

```

# Problem 3

### Write a function

Using this code chunk to write a function that use `t.test` to calculate the estimate and p_value.

```{r}
fn_t_test = function(n_obs =30, mu = 0, sigma = 5){
  
  fn_data = rnorm(n = n_obs,mean = mu, sd = sigma)
  
  tibble(fn_data)
  
  fn_data %>% 
    t.test() %>% 
    broom::tidy() %>% 
    select(estimate, p_value)
}
```








